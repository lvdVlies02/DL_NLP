{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ce9fd0",
   "metadata": {},
   "source": [
    "\n",
    "# ‚û°Ô∏è **LAB01:** develop multi-layered perceptrons, activation functions, and loss functions from scratch\n",
    "\n",
    "In this computer assignment, you will:\n",
    "- Implement common **activation functions** from scratch.\n",
    "- **Visualize** their behavior.\n",
    "- Develop a simple **Multi-Layer Perceptron (MLP)**.\n",
    "- Choose and implement the appropriate loss function.\n",
    "- Train the MLP to solve the **XOR problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f75bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.lab01 import *\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb572f54",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úçÔ∏è Task 1: Implement activation functions\n",
    "\n",
    "Complete the formulas for **Sigmoid**, **Tanh**, **ReLU**, and **Leaky ReLU** below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO: Implement the Sigmoid function\n",
    "    return \"FILL_IN\"\n",
    "\n",
    "def tanh(x):\n",
    "    # TODO: Implement the Tanh function\n",
    "    return \"FILL_IN\"\n",
    "\n",
    "def relu(x):\n",
    "    # TODO: Implement the ReLU function\n",
    "    return \"FILL_IN\"\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # TODO: Implement the Leaky ReLU function\n",
    "    return \"FILL_IN\"\n",
    "\n",
    "def elu(x, alpha=0.01):\n",
    "    # TODO: Implement the ELU function\n",
    "    return \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "        \"Sigmoid\": sigmoid,\n",
    "        \"Tanh\": tanh,\n",
    "        \"ReLU\": relu,\n",
    "        \"Leaky ReLU\": lambda x: leaky_relu(x, alpha=0.1),\n",
    "        \"ELU\": lambda x: elu(x, alpha=1.0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a2c2d",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Task 2: Visualize activation functions\n",
    "\n",
    "Use the helper function below to visualize the implemented activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf196955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_act_fn():\n",
    "    x = torch.linspace(-5, 5, 200)\n",
    "    \n",
    "    for name, act in activations.items():\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        try:\n",
    "            y = act(x)\n",
    "            y_grads = get_grads(act, x)\n",
    "            plt.plot(x, y, linewidth=3, label=name)\n",
    "            plt.plot(x, y_grads, linewidth=3, label=f\"Gradient of {name}\")\n",
    "        except:\n",
    "            plt.plot([], [], label=f\"{name} (not implemented)\")\n",
    "        plt.title(name)\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"f(x)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize the activation functions\n",
    "vis_act_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db950f88",
   "metadata": {},
   "source": [
    "\n",
    "## üß© Task 3: Create the neural network\n",
    "\n",
    "**Create a multi-layer Perceptron**: In the `NeuralNetwork` class below, the code is missing for the affine linear transformation of the input layer and the output layer . At the line `self.linear1` and `self.linear2`, replace `FILL_IN` by a linear transformation with the correct parameters. Please refer to the [PyTorch nn documentation](https://pytorch.org/docs/stable/nn.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129827bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, act_fn):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Implement an affine linear transformation with the correct parameters\n",
    "        self.linear1 = \"FILL_IN\"\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # TODO: Implement an affine linear transformation with the correct parameters\n",
    "        self.linear2 = \"FILL_IN\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbedfd21",
   "metadata": {},
   "source": [
    "\n",
    "## üßÆ Task 4: Choose and implement the appropriate loss function\n",
    "\n",
    "Decide which loss function to use for this application. Implement the loss function from scratch in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, targets):\n",
    "    # TODO: Implement the loss function\n",
    "    loss = \"FILL_IN\"\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a8c00",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Task 5: Train and evaluate the neural network on XOR dataset\n",
    "\n",
    "Try out different activation functions and observe how they influence the performance and decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, act in activations.items():\n",
    "    print(f\"\\n--- Training with {name} ---\")\n",
    "    model = NeuralNetwork(2, 4, 1, act_fn=act).to(device)\n",
    "    loss_module =  loss_wrapper(loss_function)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset = XORDataset(size=2500)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    train_model(model, optimizer, train_loader, loss_module, device)\n",
    "\n",
    "    # Test dataset\n",
    "    test_dataset = XORDataset(size=500)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    predictions, accuracy = eval_model(model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    _ = visualize_boundary(model, test_dataset.data, predictions, device, f\"{name} Decision Boundary\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae75cee",
   "metadata": {},
   "source": [
    "\n",
    "## üí≠ Summary & Reflection\n",
    "\n",
    "After completing this exercise, you should be able to:\n",
    "\n",
    "- Understand why **non-linear activation functions** are essential in neural networks.\n",
    "- See how **different activation functions** affect model learning and decision boundaries.\n",
    "- Implement and visualize activation functions from their mathematical formulas.\n",
    "\n",
    "**Discussion:**  \n",
    "Which activation function worked best for the XOR problem? Why do you think that is?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
