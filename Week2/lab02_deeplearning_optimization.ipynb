{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ➡️ **LAB02:** Optimization algorithms\n",
    "\n",
    "In this exercise, you're going to implement different optimization algorithms in Pytorch from scratch. Optimization algorithms are at the core of deep learning, as they power the training of models by minimizing loss functions and finding the best parameters.\n",
    "\n",
    "In this session, you’ll:\n",
    "- **Understand** the mathematical foundations of 5 commonly used optimization algorithms.\n",
    "- **Implement** the optimization algorithms into Pytorch.\n",
    "- **Compare** the accuracies of the 5 optimization algorithms. \n",
    "\n",
    "By the end of this session, you will not only understand *how* these algorithms work but also gain a deeper appreciation of their *pros* and *cons* when training neural networks. Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "Let's start with importing the necessary software libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from utils.lab02 import *\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## check whether we are working on MAC or else (Windows, Linux)\n",
    "if platform.system() == \"Darwin\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset loaders\n",
    "Download or load the MNIST dataset, and make a dataloader for Pytorch\n",
    "\n",
    "The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), each 28×28 pixels. It is split into 60,000 training images and 10,000 test images. Its creation played a crucial role in advancing convolutional neural networks (CNNs), as Yann LeCun's work on MNIST helped establish CNNs as a powerful tool for image recognition.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/3f774dcd5fee6c64e7ddef90158db6d68ac72405ad5a8a705c09ebdcf4f99192/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network, loss function, and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. The code for the first optimizer, **Stochastic Gradient Descent (SGD)**, has been prepared. Read the text, inspect the code and run it.\n",
    "\n",
    "2. For the remaining optimization algorithms read the text, interpret the equations and implement the algorithm from scratch using the **SGD** code as example:\n",
    "    1. **SGD with Momentum**\n",
    "\n",
    "    2. **Adagrad**\n",
    "\n",
    "    3. **RMSProp**\n",
    "    \n",
    "    4. **Adam**\n",
    "    \n",
    "3. Evaluate the performance of the 5 optimization algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer 1:** Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic gradient descent is the simplest optimization algorithm: we take our current model parameters $\\theta_t$ and subtract the gradient of those parameters, $\\nabla_\\theta \\mathcal{L}(\\theta_t)$, multiplied by the learning rate, $\\eta$. \n",
    "\n",
    "The learning rate is an important hyperparameter that controls the magnitude of the parameter update. If our learning rate is too small then our parameter updates will also be too small for us to train our model in a reasonable amount of time. Conversely, if our learning rate is too large then the size of the parameter updates will be so large that learning will become unstable! If you ever get a `NaN` value for your loss, one of the first things to try would be lowering the learning rate.\n",
    "\n",
    "The SGD algorithm is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t)$$\n",
    "\n",
    "In this notebook we'll use the term SGD, but we're formally implementing the **mini-batch gradient descent**. The general rule of thumb is that nowadays when someone mentions stochastic gradient descent then they mean mini-batch gradient descent. \n",
    "\n",
    "In PyTorch, the optimizer is called **SGD** even though it can do any of the gradient descent variants. In the code below the PyTorch methods with a trailing underscore, e.g., `.add_`, means the operation is in-place which overwrites the input tensor without keeping its previous values, thus saving memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(OptimizerTemplate):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params, lr)\n",
    "        \n",
    "    def update_param(self, p):\n",
    "        p_update = -self.lr * p.grad\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network with the SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim=28*28, hidden_dim=100, output_dim=10)\n",
    "optimizer = SGD(model.parameters(), lr=1e-2)\n",
    "accuracy = train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "accuracies[\"SGD\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer 2:** SGD with Momentum\n",
    "\n",
    "One way to think of SGD is a ball rolling down a hill, where areas of high gradient are steep parts of the hill and areas of low gradient are very flat areas. Sometimes the global minima, the point with the lowest loss, is in the middle of a giant flat area. The problem is that because these flat areas have small gradients they also give small update steps which makes learning slow. \n",
    "\n",
    "We'd want to add something to our optimizer that made it keep the \"momentum\" gained rolling down the steep hills whilst it's going across the flat areas. \n",
    "\n",
    "That's exact what SGD with momentum does! Our parameter update is now calculated using a velocity, $v$, which depends on previous velocity multiplied by the momentum $\\beta$. Commonly used momentum values are usually around 0.9ish. Here were are implementing the version where the gradient contribution is scaled relative to the momentum factor (omitting the learning rate inside the velocity term):\n",
    "\n",
    "\\begin{align*}\n",
    "    v_{t} &= \\beta \\cdot v_{t-1} + (1-\\beta) \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_{t})\\\\\n",
    "    \\theta_{t+1} &= \\theta_{t} - \\eta \\cdot v_{t}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tips:** \n",
    "- `self.param_momentum` is a dictionary that keeps track of the \"momentum\" (the moving average of gradients) for each parameter.\n",
    "\n",
    "- Each time `update_param` is called, the momentum should be updated with the current gradient `p.grad`, and it should overwrite the `self.param_momentum[p]`: it is the running average of gradients, therefore `=` suffices in the code below.\n",
    "\n",
    "- The momentum term ($v_{t}$) helps the optimizer \"remember\" past gradients, leading to smoother updates and potentially faster convergence.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: implement SGD with momentum in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, beta=0.9):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta = beta\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # dictionary to store the model parameters as key \"p\"\n",
    "        \n",
    "    def update_param(self, p):\n",
    "        self.param_momentum[p] = \"FILL_IN\"\n",
    "        p_update = \"FILL_IN\"\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network with SGD with Momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim=28*28, hidden_dim=100, output_dim=10)\n",
    "optimizer = SGDMomentum(model.parameters(), lr=1e-2)\n",
    "accuracy = train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "accuracies[\"SGD+Momentum\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer 3:** Adagrad\n",
    "\n",
    "One downside with SGD is that we use a single learning rate across all of our parameters, and that this learning rate is fixed through the entirety of training. Ideally, parameters that are updated more frequently have a lower learning rate and parameters that are updated infrequently have a larger learning rate. \n",
    "\n",
    "This is what Adagrad does. Adagrad makes the learning rate smaller for parameters that get updated often (large gradients) and keeps the learning rate larger for parameters that are updated less frequently (small gradients). Adagrad’s ability to adjust learning rates ensures that sparse features (i.e., not frequently used features) are not neglected. Sparse features can be key to making sense of the data. \n",
    "\n",
    "We introduce $G_{t}$ which holds the accumulated gradients for the entire update history. $G_{t}$ is the sum of the squared gradients up to, and including, time-step $t$. $G_{t}$ is initialized to some value, usually zero by default. As the square of the gradients of a parameter are accumulated, $G_{t}$ increases, and thus reduces the learning rate for that parameter. Because of the accumulated sum of the squared gradients, we should use `+=` in the code below.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t}}+\\epsilon} \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_{t})$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "G_{t} = G_{t-1} + \\big(\\nabla_\\theta \\mathcal{L}(\\theta_{t})\\big)^2\n",
    "$$ \n",
    "\n",
    "or:\n",
    "\n",
    "$$\n",
    "G_{t} = \\big(\\nabla_{\\theta_1} \\mathcal{L}(\\theta_{1})\\big)^2 + \\big(\\nabla_{\\theta_2} \\mathcal{L}(\\theta_{2})\\big)^2 + ... + \\big(\\nabla_{\\theta_t} \\mathcal{L}(\\theta_{t})\\big)^2\n",
    "$$\n",
    "\n",
    "$\\epsilon$ is very small number, used to avoid division by zero in the denominator. Sometimes you'll see $\\epsilon$ inside the square root, and sometimes it will be outside. PyTorch leaves it outside so we will too.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: implement Adagrad in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, eps=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.eps = eps\n",
    "        self.param_squared_grad_sum = {p: torch.zeros_like(p.data) for p in self.params} # dictionary to store the model parameters as key \"p\"\n",
    "    \n",
    "    def update_param(self, p):\n",
    "        self.param_squared_grad_sum[p] += \"FILL_IN\"\n",
    "        p_update = \"FILL_IN\"\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network with Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim=28*28, hidden_dim=100, output_dim=10)\n",
    "optimizer = Adagrad(model.parameters(), lr=1e-2)\n",
    "accuracy = train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "accuracies[\"Adagrad\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer 4:** RMSProp\n",
    "\n",
    "A drawback of Adagrad is that it takes the entire update history into account, meaning that the $G_t$ term monotonically increases over time, and the learning rate decays to 0 (i.e., no learning at some point).\n",
    "\n",
    "Root mean square propagation (RMSProp) solves this issue, as it only considers the most recent update history using an exponential weighted average (like the one in momentum). \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_{t}} + \\epsilon} \\cdot \\nabla_{\\theta_t} \\mathcal{L}(\\theta_{t})$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$v_{t} = \\beta \\cdot v_{t-1} + (1 - \\beta) \\cdot \\big(\\nabla_\\theta \\mathcal{L}(\\theta_{t})\\big)^2$$\n",
    "\n",
    "In PyTorch, they move the $\\epsilon$ term back outside of the square root, similar to Adagrad.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: implement RMSProp in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, beta=0.9, eps=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.param_2nd_momentum = {p: torch.zeros_like(p.data) for p in self.params} # dictionary to store the model parameters as key \"p\"\n",
    "    \n",
    "    def update_param(self, p):\n",
    "        self.param_2nd_momentum[p] = \"FILL_IN\"\n",
    "        p_update = \"FILL_IN\"\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network with RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim=28*28, hidden_dim=100, output_dim=10)\n",
    "optimizer = RMSProp(model.parameters(), lr=1e-2)\n",
    "accuracy = train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "accuracies[\"RMSProp\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer 5:** Adam\n",
    "\n",
    "RMSprop's idea of annealing the step size over time, whilst using an exponential moving average to avoid saturation, seemed to work out OK. What if we added momentum to it? That's how you get Adam.\n",
    "\n",
    "Adam has an exponential moving average of the gradients, like the momentum term that can be added to SGD, and an exponential moving average of squared gradients, like RMSprop.\n",
    "\n",
    "\\begin{align*}\n",
    "    m_{t} &= \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_{t}) \\\\\n",
    "    v_{t} &= \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot \\big(\\nabla_\\theta \\mathcal{L}(\\theta_{t})\\big)^2\n",
    "\\end{align*}\n",
    "\n",
    "As $m$ and $v$ are initialized to zero and $\\beta_1$ and $\\beta_2$ are initialized close to one the $m$ and $v$ values calculated on the first few update steps are \"biased\" towards very small values. This is causing huge losses for the first steps when using Adagrad, or RMSprop. \n",
    "\n",
    "To solve this, Adam uses \"bias corrected\" values of $m$ and $v$, calculated as:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{m}_{t} &= \\frac{m_{t}}{1-\\beta_1^t} \\\\\n",
    "    \\hat{v}_{t} &= \\frac{v_{t}}{1-\\beta_2^t}\n",
    "\\end{align*}\n",
    "\n",
    "This gives the final Adam equation as:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_{t}}}+\\epsilon} \\cdot \\hat{m_{t}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: implement Adam in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.param_step = {p: 0 for p in self.params} # Remembers \"t\" for each parameter for bias correction\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # dictionary to store the model parameters as key \"p\"\n",
    "        self.param_2nd_momentum = {p: torch.zeros_like(p.data) for p in self.params} # dictionary to store the model parameters as key \"p\"\n",
    "        \n",
    "    def update_param(self, p):\n",
    "        self.param_step[p] += 1\n",
    "        \n",
    "        self.param_momentum[p] = \"FILL_IN\"\n",
    "        self.param_2nd_momentum[p] = \"FILL_IN\"\n",
    "        \n",
    "        bias_correction_1 = \"FILL_IN\"\n",
    "        bias_correction_2 = \"FILL_IN\"\n",
    "     \n",
    "        p_update = \"FILL_IN\"\n",
    "        \n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim=28*28, hidden_dim=100, output_dim=10)\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)\n",
    "accuracy = train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "accuracies[\"Adam\"] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the accuracies of the different optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for opt_name in accuracies:\n",
    "    plt.plot(accuracies[opt_name], label=opt_name)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Comparison of Optimizer Performance on MNIST\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ➡️ **LAB02:** Regularization\n",
    "\n",
    "In this exercise, you're going to implement and use different regularization techniques in PyTorch.\n",
    "\n",
    "You will explore how regularization techniques help improve deep learning models by preventing overfitting and enhancing generalization. Regularization plays a crucial role in training robust models by constraining complexity and reducing dependency on specific patterns in the training data.\n",
    "\n",
    "In this session, you’ll:\n",
    "\n",
    "1. Split a training dataset into a training set and validation set\n",
    "\n",
    "2. Implement 5 commonly used regularization techniques in Pytorch, and compare their impact on model performance and generalization:\n",
    "    1. **L1 Regularization (Lasso)**\n",
    "\n",
    "    2. **L2 Regularization (Ridge)**\n",
    "\n",
    "    3. **Dropout**\n",
    "    \n",
    "    4. **Batch Normalization**\n",
    "\n",
    "    5. **Early Stopping**\n",
    "\n",
    "<br>  \n",
    "By the end of this session, you will not only understand *how* these techniques work but also gain insights into their strengths and limitations when training neural networks. \n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Validation split\n",
    "\n",
    "The MNIST dataset doesn't have a **validation dataset** yet. A validation dataset is needed to monitor the model's performance during training, so that we can determine when to tune hyperparameters or when to stop training, ensuring the model doesn't overfit to the training data.\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: split the original training dataset into a training subset of 80% and a new validation set of 20%**\n",
    "\n",
    "Find inspiration on the [PyTorch website](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = \"FILL_IN\"\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline neural network model\n",
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(28 * 28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 64),        \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 10)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Function to Optimize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, trainloader, valloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    ## Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(trainloader), val_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Function to Assess Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_model(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return test_loss / len(testloader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Plot Function to Plot Training and Test Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training loss curve\n",
    "def plot_loss_curve(epochs, train_losses, val_losses, test_losses, title=\"\"):\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Val Loss')\n",
    "    plt.plot(range(1, epochs+1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Train, Validation and Test Loss Curve - {title}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training, Evaluation, and Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BaselineNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_loss_curve(num_epochs, train_losses, val_losses, test_losses, title=\"Standard model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - L1 Regularization (Lasso)\n",
    "\n",
    "Network overfitting can be caused by the model being too complex or overly sensitive to specific patterns in the training data.\n",
    "This can be caused by large network weights, meaning that the model is highly responsive to small changes in input, making it sensitive to noise and outliers.\n",
    "\n",
    "L1 regularization adds the absolute value of the model parameters (weights) as a penalty to the loss function, to encourage the model to learn smaller, simpler weights that generalize better to new data. L1 regularization encourages sparsity in the model by forcing some weights to become exactly zero, effectively selecting only the most important features. The equation is:\n",
    "\n",
    "$$\n",
    "L_1 = \\lambda \\sum_{i} \\sum_{j} | w_{i,j} |\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ (lambda_reg) is a scaling factor that determines how much regularization is applied.\n",
    "- $i$ indexes over all **layers** in the model.\n",
    "- $j$ indexes over all **weights** in a given layer.\n",
    "- $| w_{i,j} |$ represents the absolute value of each weight.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tip:** In PyTorch, **mathematical operations** are native to tensors, much like NumPy arrays. Please refer to the [PyTorch Pointwise Ops](https://pytorch.org/docs/stable/torch.html#pointwise-ops) and [PyTorch Reduction Ops](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: Implement L1 regularization in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(model, lambda_reg):\n",
    "    l1_penalty = \"FILL_IN\"\n",
    "    return lambda_reg * l1_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - L2 Regularization (Ridge)\n",
    "\n",
    "L2 regularization adds the squared value of the model parameters as a penalty to the loss function. L2 regularization prevents large weight values and encourages smaller, more evenly distributed weights, leading to better generalization. This is the equation:\n",
    "\n",
    "$$\n",
    "L_2 = \\lambda \\sum_{i} \\sum_{j} w_{i,j}^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ (lambda_reg) is a scaling factor that determines how much regularization is applied.\n",
    "- $i$ indexes over all **layers** in the model.\n",
    "- $j$ indexes over all **weights** in a given layer.\n",
    "- $w_{i,j}^2$ represents the squared value of each weight.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tip:** In PyTorch, **mathematical operations** are native to tensors, much like NumPy arrays. Please refer to the [PyTorch Pointwise Ops](https://pytorch.org/docs/stable/torch.html#pointwise-ops) and [PyTorch Reduction Ops](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: Implement L2 regularization in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(model, lambda_reg):\n",
    "    l2_penalty = \"FILL_IN\"\n",
    "    return lambda_reg * l2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next year: build Elastic Net by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda, l2_lambda = 0.001, 0.0001\n",
    "l1_reg = sum(p.abs().sum() for p in model.parameters())\n",
    "l2_reg = sum((p ** 2).sum() for p in model.parameters())\n",
    "loss = data_loss + l1_lambda * l1_reg + l2_lambda * l2_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 & 2.2 - L1 and L2 regularization as parameter penalties\n",
    "\n",
    "**To Do: Add the L1 and L2 regularization as components to the loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_w_regularization(model, trainloader, valloader, criterion, optimizer, regularization_type, lambda_reg):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if regularization_type == 'L1':\n",
    "            loss += \"FILL_IN\"\n",
    "        if regularization_type == 'L2':\n",
    "            loss += \"FILL_IN\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    ## Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(trainloader), val_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation of the Model with L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BaselineNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 30\n",
    "lambda_reg = 1e-5\n",
    "regularization_type = 'L1'\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_model_w_regularization(model, train_loader, val_loader, criterion, optimizer, regularization_type, lambda_reg)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_loss_curve(num_epochs, train_losses, val_losses, test_losses, title=\"L1 Regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation of the Model with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BaselineNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 30\n",
    "lambda_reg = 5e-4\n",
    "regularization_type = 'L2'\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_model_w_regularization(model, train_loader, val_loader, criterion, optimizer, regularization_type, lambda_reg)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_loss_curve(num_epochs, train_losses, val_losses, test_losses, title=\"L2 Regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 - Dropout \n",
    "Dropout is a regularization technique in deep learning that randomly deactivates a fraction of neurons during training to prevent overfitting. By introducing this randomness, dropout helps improve model generalization by reducing reliance on specific neurons and encouraging redundancy in learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tip:** In the lecture slides of Lesson 2 you can find the best placement of dropout in a neural network\n",
    "\n",
    "<br>\n",
    "\n",
    "**To Do: Add Dropout to the BaselineNN neural network architecture below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNN_with_Droupout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(28 * 28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 64),        \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 10)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation of the Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BaselineNN_with_Droupout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_loss_curve(num_epochs, train_losses, val_losses, test_losses, title=\"Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 - Early Stopping \n",
    "Early Stopping is a regularization technique used to prevent overfitting by monitoring the model’s performance on a validation set. Training stops when the validation loss stops improving for a predefined number of epochs, ensuring the model does not overfit to the training data.\n",
    "\n",
    "The EarlyStopping class below helps to prevent overfitting by monitoring the validation loss during training. If the validation loss does not improve for a set number of epochs (defined by **patience**), training is stopped early. This ensures that the model does not continue to overfit the training data. The class tracks the best model state and restores it once early stopping is triggered, preserving the best-performing model. The **delta**  parameter allows for a minimum change in loss to be considered an improvement.\n",
    "\n",
    "Source: [early-stopping-pytorch](https://github.com/Bjarten/early-stopping-pytorch/blob/main/early_stopping_pytorch/early_stopping.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        # Check if validation loss is nan\n",
    "        if np.isnan(val_loss):\n",
    "            self.trace_func(\"Validation loss is NaN. Ignoring this epoch.\")\n",
    "            return\n",
    "\n",
    "        if self.best_val_loss is None:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss < self.best_val_loss - self.delta:\n",
    "            # Significant improvement detected\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0  # Reset counter since improvement occurred\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation of the Model with Dropout and Early Stopping\n",
    "\n",
    "**To Do: Integrate the EarlyStopping class with the appropriate inputs in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BaselineNN_with_Droupout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# EarlyStopping setup\n",
    "early_stopping = \"FILL_IN\"\n",
    "\n",
    "num_epochs = 100\n",
    "epochs_trained = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "# Train and evaluate the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")\n",
    "\n",
    "    epochs_trained += 1\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_loss_curve(epochs_trained, train_losses, val_losses, test_losses, title=\"Dropout, Early Stopping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
